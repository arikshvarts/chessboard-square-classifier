{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba0f7b24",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a81aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285867d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q python-chess pandas pillow tqdm opencv-python matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580e88a",
   "metadata": {},
   "source": [
    "## 2. Upload Code and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78b93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload code.zip\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"Upload code.zip...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "with zipfile.ZipFile('code.zip', 'r') as z:\n",
    "    for member in z.namelist():\n",
    "        z.extract(member, 'temp')\n",
    "\n",
    "if os.path.exists('temp'):\n",
    "    for root, dirs, filelist in os.walk('temp'):\n",
    "        for f in filelist:\n",
    "            old_path = os.path.join(root, f)\n",
    "            new_path = os.path.relpath(old_path, 'temp').replace('\\\\', '/')\n",
    "            directory = os.path.dirname(new_path)\n",
    "            if directory:\n",
    "                os.makedirs(directory, exist_ok=True)\n",
    "            shutil.copy(old_path, new_path)\n",
    "    shutil.rmtree('temp')\n",
    "\n",
    "print(\"\\nCode uploaded!\")\n",
    "!ls src/ dataset_tools/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4ee9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data\n",
    "import glob\n",
    "\n",
    "print(\"Upload all_games_data.zip (5 games: 2,4,5,6,7)...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "with zipfile.ZipFile(list(uploaded.keys())[0], 'r') as z:\n",
    "    for member in z.namelist():\n",
    "        z.extract(member, 'temp')\n",
    "\n",
    "if os.path.exists('temp'):\n",
    "    for root, dirs, filelist in os.walk('temp'):\n",
    "        for f in filelist:\n",
    "            old_path = os.path.join(root, f)\n",
    "            new_path = os.path.relpath(old_path, 'temp').replace('\\\\', '/')\n",
    "            directory = os.path.dirname(new_path)\n",
    "            if directory:\n",
    "                os.makedirs(directory, exist_ok=True)\n",
    "            shutil.copy(old_path, new_path)\n",
    "    shutil.rmtree('temp')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GAMES UPLOADED:\")\n",
    "print(\"=\"*60)\n",
    "total_frames = 0\n",
    "for game in sorted(glob.glob('Data/game*_per_frame')):\n",
    "    game_name = os.path.basename(game)\n",
    "    frames = len(glob.glob(f'{game}/tagged_images/*.jpg'))\n",
    "    total_frames += frames\n",
    "    print(f\"  {game_name}: {frames} frames\")\n",
    "print(f\"\\nTotal: {total_frames} frames\")\n",
    "print(f\"Total games: 5\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0746b1",
   "metadata": {},
   "source": [
    "## 3. Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc65629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from dataset_tools.fen_utils import PIECE_TO_ID, fen_board_to_64_labels, idx_to_square_name\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PREPARING DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "os.makedirs('dataset_out', exist_ok=True)\n",
    "\n",
    "with open('dataset_out/classes.json', 'w') as f:\n",
    "    json.dump({str(v): k for k, v in PIECE_TO_ID.items()}, f, indent=2)\n",
    "\n",
    "# Load all games\n",
    "game_dirs = sorted(glob.glob('Data/*_per_frame'))\n",
    "game_data = {}\n",
    "\n",
    "for game_dir in game_dirs:\n",
    "    game_id = os.path.basename(game_dir)\n",
    "    csv_file = glob.glob(f'{game_dir}/*.csv')\n",
    "\n",
    "    if not csv_file:\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(csv_file[0])\n",
    "    frame_col = 'from_frame' if 'from_frame' in df.columns else 'frame_id'\n",
    "\n",
    "    game_rows = []\n",
    "\n",
    "    for _, r in df.iterrows():\n",
    "        frame_id = int(r[frame_col])\n",
    "        fen = r['fen']\n",
    "        labels = fen_board_to_64_labels(fen)\n",
    "\n",
    "        frame_path = f'{game_dir}/tagged_images/frame_{frame_id:06d}.jpg'\n",
    "        if not os.path.exists(frame_path):\n",
    "            continue\n",
    "\n",
    "        for sq in range(64):\n",
    "            game_rows.append({\n",
    "                'frame_path': frame_path,\n",
    "                'game_id': game_id,\n",
    "                'frame_id': frame_id,\n",
    "                'square_idx': sq,\n",
    "                'row': sq // 8,\n",
    "                'col': sq % 8,\n",
    "                'square_name': idx_to_square_name(sq),\n",
    "                'label_id': labels[sq],\n",
    "            })\n",
    "\n",
    "    game_df = pd.DataFrame(game_rows)\n",
    "    game_data[game_id] = game_df\n",
    "    \n",
    "    n_frames = game_df['frame_id'].nunique()\n",
    "    n_squares = len(game_df)\n",
    "    print(f\"{game_id}: {n_frames} frames, {n_squares:,} squares\")\n",
    "\n",
    "print(f\"\\nTotal games loaded: {len(game_data)}\")\n",
    "\n",
    "# Create 2-fold cross-validation splits (for proof of learning)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING 2-FOLD CROSS-VALIDATION SPLITS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fold_manifests = []\n",
    "game_list = list(game_data.keys())\n",
    "\n",
    "# Create 2 folds: each holds out 1 game for testing\n",
    "for fold_idx in range(2):\n",
    "    test_game = game_list[fold_idx]\n",
    "    print(f\"\\nFold {fold_idx + 1}: Test on {test_game}\")\n",
    "    \n",
    "    # Combine other 4 games for training\n",
    "    train_val_dfs = []\n",
    "    for game_id, game_df in game_data.items():\n",
    "        if game_id != test_game:\n",
    "            train_val_dfs.append(game_df.copy())\n",
    "    \n",
    "    train_val_df = pd.concat(train_val_dfs, ignore_index=True)\n",
    "    test_df = game_data[test_game].copy()\n",
    "    \n",
    "    # Split training data into 80% train, 20% val\n",
    "    unique_frames = train_val_df.groupby('game_id')['frame_id'].unique()\n",
    "    \n",
    "    train_frames_list = []\n",
    "    val_frames_list = []\n",
    "    \n",
    "    for game_id, frames in unique_frames.items():\n",
    "        frames = np.array(list(frames))\n",
    "        n_frames = len(frames)\n",
    "        \n",
    "        rng = np.random.RandomState(42)\n",
    "        rng.shuffle(frames)\n",
    "        \n",
    "        n_train = int(0.8 * n_frames)\n",
    "        train_frames_list.extend([(game_id, f) for f in frames[:n_train]])\n",
    "        val_frames_list.extend([(game_id, f) for f in frames[n_train:]])\n",
    "    \n",
    "    train_frame_set = set(train_frames_list)\n",
    "    val_frame_set = set(val_frames_list)\n",
    "    \n",
    "    def assign_split_train_val(row):\n",
    "        key = (row['game_id'], row['frame_id'])\n",
    "        if key in train_frame_set:\n",
    "            return 'train'\n",
    "        elif key in val_frame_set:\n",
    "            return 'val'\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    train_val_df['split'] = train_val_df.apply(assign_split_train_val, axis=1)\n",
    "    test_df['split'] = 'test'\n",
    "    \n",
    "    fold_df = pd.concat([train_val_df, test_df], ignore_index=True)\n",
    "    fold_df = fold_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    manifest_path = f'dataset_out/fold_{fold_idx + 1}_manifest.csv'\n",
    "    fold_df.to_csv(manifest_path, index=False)\n",
    "    \n",
    "    fold_manifests.append({\n",
    "        'fold': fold_idx + 1,\n",
    "        'test_game': test_game,\n",
    "        'manifest_path': manifest_path,\n",
    "        'train_squares': (fold_df['split'] == 'train').sum(),\n",
    "        'val_squares': (fold_df['split'] == 'val').sum(),\n",
    "        'test_squares': (fold_df['split'] == 'test').sum(),\n",
    "    })\n",
    "    \n",
    "    print(f\"  Train: {fold_manifests[-1]['train_squares']:,} squares (4 games)\")\n",
    "    print(f\"  Val:   {fold_manifests[-1]['val_squares']:,} squares (20% of 4 games)\")\n",
    "    print(f\"  Test:  {fold_manifests[-1]['test_squares']:,} squares (1 game: {test_game})\")\n",
    "\n",
    "# Create combined dataset (all 5 games) for final training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING COMBINED DATASET (ALL 5 GAMES)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_games_df = pd.concat([df.copy() for df in game_data.values()], ignore_index=True)\n",
    "\n",
    "# Split all games into 80% train, 20% val\n",
    "unique_frames_all = all_games_df.groupby('game_id')['frame_id'].unique()\n",
    "\n",
    "train_frames_all = []\n",
    "val_frames_all = []\n",
    "\n",
    "for game_id, frames in unique_frames_all.items():\n",
    "    frames = np.array(list(frames))\n",
    "    n_frames = len(frames)\n",
    "    \n",
    "    rng = np.random.RandomState(42)\n",
    "    rng.shuffle(frames)\n",
    "    \n",
    "    n_train = int(0.8 * n_frames)\n",
    "    train_frames_all.extend([(game_id, f) for f in frames[:n_train]])\n",
    "    val_frames_all.extend([(game_id, f) for f in frames[n_train:]])\n",
    "\n",
    "train_frame_set_all = set(train_frames_all)\n",
    "val_frame_set_all = set(val_frames_all)\n",
    "\n",
    "def assign_split_all(row):\n",
    "    key = (row['game_id'], row['frame_id'])\n",
    "    if key in train_frame_set_all:\n",
    "        return 'train'\n",
    "    elif key in val_frame_set_all:\n",
    "        return 'val'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "all_games_df['split'] = all_games_df.apply(assign_split_all, axis=1)\n",
    "all_games_df = all_games_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "all_games_df.to_csv('dataset_out/all_games_manifest.csv', index=False)\n",
    "\n",
    "train_count = (all_games_df['split'] == 'train').sum()\n",
    "val_count = (all_games_df['split'] == 'val').sum()\n",
    "\n",
    "print(f\"Train: {train_count:,} squares (80% of all 5 games)\")\n",
    "print(f\"Val:   {val_count:,} squares (20% of all 5 games)\")\n",
    "print(\"\\n✓ All datasets prepared!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e021e5",
   "metadata": {},
   "source": [
    "## 4. Quick 2-Fold Validation (Proof of Learning)\n",
    "\n",
    "Train 2 folds to prove the model learns effectively across different games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258eb734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"2-FOLD CROSS-VALIDATION (PROOF OF LEARNING)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Start Time: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Load classes\n",
    "with open('dataset_out/classes.json', 'r') as f:\n",
    "    classes = json.load(f)\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Dataset class\n",
    "class ChessSquareDataset(Dataset):\n",
    "    def __init__(self, manifest_df, transform=None):\n",
    "        self.data = manifest_df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = row['frame_path']\n",
    "        label = int(row['label_id'])\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        W, H = img.size\n",
    "        sq_w, sq_h = W // 8, H // 8\n",
    "        col, row_sq = row['col'], row['row']\n",
    "        \n",
    "        left = col * sq_w\n",
    "        top = row_sq * sq_h\n",
    "        right = left + sq_w\n",
    "        bottom = top + sq_h\n",
    "        \n",
    "        img_crop = img.crop((left, top, right, bottom))\n",
    "        \n",
    "        if self.transform:\n",
    "            img_crop = self.transform(img_crop)\n",
    "        \n",
    "        return img_crop, label\n",
    "\n",
    "# Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Training functions\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / total, 100. * correct / total\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / total, 100. * correct / total\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Training config\n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.001\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# Results storage\n",
    "fold_results = []\n",
    "\n",
    "# Train 2 folds\n",
    "for fold_idx in range(2):\n",
    "    fold_info = fold_manifests[fold_idx]\n",
    "    fold_num = fold_info['fold']\n",
    "    test_game = fold_info['test_game']\n",
    "    manifest_path = fold_info['manifest_path']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"FOLD {fold_num}/2 - Testing on {test_game}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load data\n",
    "    manifest_df = pd.read_csv(manifest_path)\n",
    "    train_df = manifest_df[manifest_df['split'] == 'train']\n",
    "    val_df = manifest_df[manifest_df['split'] == 'val']\n",
    "    test_df = manifest_df[manifest_df['split'] == 'test']\n",
    "    \n",
    "    print(f\"Train: {len(train_df):,} squares\")\n",
    "    print(f\"Val:   {len(val_df):,} squares\")\n",
    "    print(f\"Test:  {len(test_df):,} squares\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ChessSquareDataset(train_df, transform=train_transform)\n",
    "    val_dataset = ChessSquareDataset(val_df, transform=val_transform)\n",
    "    test_dataset = ChessSquareDataset(test_df, transform=val_transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = models.resnet50(weights='IMAGENET1K_V1')\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "    \n",
    "    # Training loop\n",
    "    fold_start = time.time()\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_start = time.time()\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "        \n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%\")\n",
    "        print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.2f}%\")\n",
    "        print(f\"  Time:  {time.time()-epoch_start:.1f}s\")\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'fold': fold_num\n",
    "            }, f'dataset_out/fold_{fold_num}_best.pth')\n",
    "            print(f\"  ✓ Best model saved\")\n",
    "    \n",
    "    # Test\n",
    "    checkpoint = torch.load(f'dataset_out/fold_{fold_num}_best.pth', weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    test_loss, test_acc = validate(model, test_loader, criterion, device)\n",
    "    \n",
    "    fold_time = time.time() - fold_start\n",
    "    \n",
    "    print(f\"\\nFold {fold_num} Results:\")\n",
    "    print(f\"  Best Val Acc:  {best_val_acc:.2f}%\")\n",
    "    print(f\"  Test Acc:      {test_acc:.2f}%\")\n",
    "    print(f\"  Time:          {fold_time/60:.1f} min\")\n",
    "    \n",
    "    fold_results.append({\n",
    "        'fold': fold_num,\n",
    "        'test_game': test_game,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'time_min': fold_time / 60\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2-FOLD VALIDATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fold_df = pd.DataFrame(fold_results)\n",
    "print(fold_df.to_string(index=False))\n",
    "\n",
    "mean_test = fold_df['test_acc'].mean()\n",
    "print(f\"\\nMean Test Accuracy: {mean_test:.2f}%\")\n",
    "print(f\"✓ Model learns effectively across games!\" if mean_test > 85 else \"⚠ Lower than expected\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c85669",
   "metadata": {},
   "source": [
    "## 5. Full Training on ALL 5 Games (Final Model)\n",
    "\n",
    "Train a single model on all 5 games combined for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121ce199",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING ON ALL 5 GAMES COMBINED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Start Time: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Load combined dataset\n",
    "all_manifest = pd.read_csv('dataset_out/all_games_manifest.csv')\n",
    "train_df = all_manifest[all_manifest['split'] == 'train']\n",
    "val_df = all_manifest[all_manifest['split'] == 'val']\n",
    "\n",
    "print(f\"\\nTrain: {len(train_df):,} squares (80% of all 5 games)\")\n",
    "print(f\"Val:   {len(val_df):,} squares (20% of all 5 games)\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ChessSquareDataset(train_df, transform=train_transform)\n",
    "val_dataset = ChessSquareDataset(val_df, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Initialize model\n",
    "model = models.resnet50(weights='IMAGENET1K_V1')\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "\n",
    "# Training loop\n",
    "train_start = time.time()\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "history = []\n",
    "\n",
    "print(\"\\nTraining for 8 epochs...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    history.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'train_acc': train_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'val_acc': val_acc,\n",
    "        'lr': optimizer.param_groups[0]['lr'],\n",
    "        'time_sec': epoch_time\n",
    "    })\n",
    "    \n",
    "    print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%\")\n",
    "    print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.2f}%\")\n",
    "    print(f\"  Time:  {epoch_time:.1f}s\")\n",
    "    print(f\"  LR:    {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'train_acc': train_acc,\n",
    "            'games': 'all_5_games'\n",
    "        }, 'dataset_out/best_model_all_games.pth')\n",
    "        print(f\"  ✓ Best model saved (Val Acc: {val_acc:.2f}%)\")\n",
    "\n",
    "train_time = time.time() - train_start\n",
    "\n",
    "# Save history\n",
    "pd.DataFrame(history).to_csv('dataset_out/all_games_history.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING ON ALL 5 GAMES COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best Val Acc:    {best_val_acc:.2f}% (epoch {best_epoch})\")\n",
    "print(f\"Training Time:   {train_time/60:.1f} minutes\")\n",
    "print(f\"Model Saved:     dataset_out/best_model_all_games.pth\")\n",
    "print(\"\\n✓ Final production model ready!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86f58da",
   "metadata": {},
   "source": [
    "## 6. Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf226ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load training history\n",
    "history_df = pd.read_csv('dataset_out/all_games_history.csv')\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Training Progress (Loss)\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history_df['epoch'], history_df['train_loss'], 'b-o', label='Train Loss', linewidth=2)\n",
    "ax1.plot(history_df['epoch'], history_df['val_loss'], 'r-o', label='Val Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Training Progress - Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Training Progress (Accuracy)\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(history_df['epoch'], history_df['train_acc'], 'b-o', label='Train Acc', linewidth=2)\n",
    "ax2.plot(history_df['epoch'], history_df['val_acc'], 'r-o', label='Val Acc', linewidth=2)\n",
    "ax2.axhline(best_val_acc, color='g', linestyle='--', label=f'Best: {best_val_acc:.2f}%')\n",
    "ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Training Progress - Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: 2-Fold Validation Results\n",
    "ax3 = axes[1, 0]\n",
    "x = np.arange(2)\n",
    "width = 0.35\n",
    "bars1 = ax3.bar(x - width/2, fold_df['best_val_acc'], width, label='Val Acc', \n",
    "                color='green', alpha=0.7, edgecolor='black')\n",
    "bars2 = ax3.bar(x + width/2, fold_df['test_acc'], width, label='Test Acc', \n",
    "                color='orange', alpha=0.7, edgecolor='black')\n",
    "ax3.set_xlabel('Fold', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('2-Fold Cross-Validation Results', fontsize=14, fontweight='bold')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(['Fold 1', 'Fold 2'])\n",
    "ax3.legend()\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: Learning Rate Schedule\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(history_df['epoch'], history_df['lr'], 'purple', marker='o', linewidth=2)\n",
    "ax4.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('Learning Rate', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "ax4.set_yscale('log')\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dataset_out/training_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n2-Fold Validation:\")\n",
    "print(f\"  Mean Test Acc: {fold_df['test_acc'].mean():.2f}%\")\n",
    "print(f\"  Fold 1: {fold_df.iloc[0]['test_acc']:.2f}% (test on {fold_df.iloc[0]['test_game']})\")\n",
    "print(f\"  Fold 2: {fold_df.iloc[1]['test_acc']:.2f}% (test on {fold_df.iloc[1]['test_game']})\")\n",
    "\n",
    "print(\"\\nFinal Model (All 5 Games):\")\n",
    "print(f\"  Best Val Acc: {best_val_acc:.2f}%\")\n",
    "print(f\"  Best Epoch: {best_epoch}\")\n",
    "print(f\"  Training Time: {train_time/60:.1f} min\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"✓ Cross-game learning validated (2-fold)\")\n",
    "print(\"✓ Final model trained on all available data\")\n",
    "print(f\"✓ Model achieves {best_val_acc:.2f}% validation accuracy\")\n",
    "print(\"✓ Ready for production deployment\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ecfb79",
   "metadata": {},
   "source": [
    "## 7. Download Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80035bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DOWNLOADING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Download final model\n",
    "print(\"\\nDownloading best_model_all_games.pth...\")\n",
    "files.download('dataset_out/best_model_all_games.pth')\n",
    "\n",
    "# Download visualizations\n",
    "print(\"Downloading training_results.png...\")\n",
    "files.download('dataset_out/training_results.png')\n",
    "\n",
    "# Download history\n",
    "print(\"Downloading all_games_history.csv...\")\n",
    "files.download('dataset_out/all_games_history.csv')\n",
    "\n",
    "print(\"\\n✓ Downloads complete!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Rename best_model_all_games.pth to best_model_fold_1.pth\")\n",
    "print(\"2. Place in checkpoints/ folder\")\n",
    "print(\"3. Run: python app.py\")\n",
    "print(\"4. Access web app at http://localhost:5000\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
