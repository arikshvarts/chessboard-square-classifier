{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K-5S82XPtOu7",
    "outputId": "42a2c811-76d7-4dfa-9529-304090f5cacf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xY9l88EftqKo",
    "outputId": "200ff0bf-f63c-46d9-d2a2-f290dc1d46fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/6.1 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m5.2/6.1 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for chess (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q python-chess pandas pillow tqdm opencv-python matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "1L1FgXV5t6EA",
    "outputId": "5932b134-6da3-4210-88c9-7ff4c2201e23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload code.zip...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-1e76f12f-8306-4ee5-a9c6-8cfd764f059c\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-1e76f12f-8306-4ee5-a9c6-8cfd764f059c\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving code.zip to code.zip\n",
      "\n",
      "Code uploaded!\n",
      "dataset_tools/:\n",
      "board_detect_and_warp.py  extract_squares.py  make_dataset.py\n",
      "debug_grid.py\t\t  fen_utils.py\t      __pycache__\n",
      "eval.py\t\t\t  __init__.py\t      show_crops.py\n",
      "\n",
      "src/:\n",
      "dataset.py  __init__.py  predict.py   train.py\n",
      "eval.py     model.py\t __pycache__  visualize.py\n"
     ]
    }
   ],
   "source": [
    "# //code.zip\n",
    "\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"Upload code.zip...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "with zipfile.ZipFile('code.zip', 'r') as z:\n",
    "    for member in z.namelist():\n",
    "        z.extract(member, 'temp')\n",
    "\n",
    "if os.path.exists('temp'):\n",
    "    for root, dirs, filelist in os.walk('temp'):\n",
    "        for f in filelist:\n",
    "            old_path = os.path.join(root, f)\n",
    "            new_path = os.path.relpath(old_path, 'temp').replace('\\\\', '/')\n",
    "            directory = os.path.dirname(new_path)\n",
    "            if directory:\n",
    "                os.makedirs(directory, exist_ok=True)\n",
    "            shutil.copy(old_path, new_path)\n",
    "    shutil.rmtree('temp')\n",
    "\n",
    "print(\"\\nCode uploaded!\")\n",
    "!ls src/ dataset_tools/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "c-7j4kfnt-WJ",
    "outputId": "25f09b17-7ae4-4b83-c18e-b7a52c845d37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload all_games_data.zip (games 2,4,5,6,7)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-f5b08697-2b26-459a-94b9-64873a6d8bf8\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-f5b08697-2b26-459a-94b9-64873a6d8bf8\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving all_games_data.zip to all_games_data.zip\n",
      "\n",
      "============================================================\n",
      "GAMES UPLOADED:\n",
      "============================================================\n",
      "  game2_per_frame: 77 frames\n",
      "  game4_per_frame: 184 frames\n",
      "  game5_per_frame: 109 frames\n",
      "  game6_per_frame: 92 frames\n",
      "  game7_per_frame: 55 frames\n",
      "\n",
      "Total: 517 frames\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "print(\"Upload all_games_data.zip (games 2,4,5,6,7)...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "with zipfile.ZipFile(list(uploaded.keys())[0], 'r') as z:\n",
    "    for member in z.namelist():\n",
    "        z.extract(member, 'temp')\n",
    "\n",
    "if os.path.exists('temp'):\n",
    "    for root, dirs, filelist in os.walk('temp'):\n",
    "        for f in filelist:\n",
    "            old_path = os.path.join(root, f)\n",
    "            new_path = os.path.relpath(old_path, 'temp').replace('\\\\', '/')\n",
    "            directory = os.path.dirname(new_path)\n",
    "            if directory:\n",
    "                os.makedirs(directory, exist_ok=True)\n",
    "            shutil.copy(old_path, new_path)\n",
    "    shutil.rmtree('temp')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GAMES UPLOADED:\")\n",
    "print(\"=\"*60)\n",
    "total_frames = 0\n",
    "for game in sorted(glob.glob('Data/game*_per_frame')):\n",
    "    game_name = os.path.basename(game)\n",
    "    frames = len(glob.glob(f'{game}/tagged_images/*.jpg'))\n",
    "    total_frames += frames\n",
    "    print(f\"  {game_name}: {frames} frames\")\n",
    "print(f\"\\nTotal: {total_frames} frames\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Fold Cross-Validation + Final Training Strategy\n",
    "\n",
    "### Overview\n",
    "This notebook implements a **2-fold cross-validation** to prove learning capability, followed by **training on all 5 games** for the final production model.\n",
    "\n",
    "### Phase 1: 2-Fold Cross-Validation (Proof of Learning)\n",
    "Each fold:\n",
    "- **Trains on 4 games** (with 80/20 train/val split within those 4)\n",
    "- **Tests on the 5th game** (100% of that game used for testing)\n",
    "- **Starts with fresh pretrained ResNet50 weights**\n",
    "- **Trains for 8 epochs** with early stopping based on validation accuracy\n",
    "\n",
    "**Fold 1**: Test on game7_per_frame  \n",
    "**Fold 2**: Test on game5_per_frame\n",
    "\n",
    "### Phase 2: Final Training (All Games)\n",
    "- **Trains on ALL 5 games** with 80/20 train/val split\n",
    "- **8 epochs** with same hyperparameters\n",
    "- **Produces final production model** for deployment\n",
    "\n",
    "### Key Features\n",
    "1. **Cross-Game Evaluation**: Proves the model can generalize to completely unseen games\n",
    "2. **Fair Comparison**: Each fold uses the same architecture and hyperparameters\n",
    "3. **Production Model**: Final training on all data for best real-world performance\n",
    "\n",
    "### Training Configuration\n",
    "- **Model**: ResNet50 (pretrained on ImageNet)\n",
    "- **Epochs**: 8 per fold\n",
    "- **Batch Size**: 128\n",
    "- **Learning Rate**: 0.001\n",
    "- **Optimizer**: Adam\n",
    "- **Data Augmentation**: Random horizontal flip, rotation, color jitter (training only)\n",
    "- **Input Size**: 224x224 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29ZQUJN8z8zF",
    "outputId": "55ea49c8-18ca-471b-b127-fcc1bb456179"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PER-GAME SPLIT (Each game -> 70/15/15)\n",
      "============================================================\n",
      "\n",
      "game2_per_frame: 77 frames\n",
      "  Train: 53 frames (3,392 sq)\n",
      "  Val:   11 frames (704 sq)\n",
      "  Test:  13 frames (832 sq)\n",
      "\n",
      "game4_per_frame: 184 frames\n",
      "  Train: 128 frames (8,192 sq)\n",
      "  Val:   27 frames (1,728 sq)\n",
      "  Test:  29 frames (1,856 sq)\n",
      "\n",
      "game5_per_frame: 109 frames\n",
      "  Train: 76 frames (4,928 sq)\n",
      "  Val:   16 frames (1,024 sq)\n",
      "  Test:  17 frames (1,088 sq)\n",
      "\n",
      "game6_per_frame: 92 frames\n",
      "  Train: 64 frames (4,096 sq)\n",
      "  Val:   13 frames (832 sq)\n",
      "  Test:  15 frames (960 sq)\n",
      "\n",
      "game7_per_frame: 55 frames\n",
      "  Train: 38 frames (2,496 sq)\n",
      "  Val:   8 frames (512 sq)\n",
      "  Test:  9 frames (576 sq)\n",
      "\n",
      "============================================================\n",
      "COMBINED DATASET\n",
      "============================================================\n",
      "TRAIN: 23,104 squares, 350 frames\n",
      "VAL: 4,800 squares, 75 frames\n",
      "TEST: 5,312 squares, 82 frames\n",
      "\n",
      "Total: 33,216 squares\n",
      "============================================================\n",
      "All games contribute to all three splits\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from dataset_tools.fen_utils import PIECE_TO_ID, fen_board_to_64_labels, idx_to_square_name\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PREPARING 2-FOLD CROSS-VALIDATION + ALL GAMES TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "os.makedirs('dataset_out', exist_ok=True)\n",
    "\n",
    "with open('dataset_out/classes.json', 'w') as f:\n",
    "    json.dump({str(v): k for k, v in PIECE_TO_ID.items()}, f, indent=2)\n",
    "\n",
    "# Load all games\n",
    "game_dirs = sorted(glob.glob('Data/*_per_frame'))\n",
    "game_data = {}\n",
    "\n",
    "for game_dir in game_dirs:\n",
    "    game_id = os.path.basename(game_dir)\n",
    "    csv_file = glob.glob(f'{game_dir}/*.csv')\n",
    "\n",
    "    if not csv_file:\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(csv_file[0])\n",
    "    frame_col = 'from_frame' if 'from_frame' in df.columns else 'frame_id'\n",
    "\n",
    "    game_rows = []\n",
    "\n",
    "    for _, r in df.iterrows():\n",
    "        frame_id = int(r[frame_col])\n",
    "        fen = r['fen']\n",
    "        labels = fen_board_to_64_labels(fen)\n",
    "\n",
    "        frame_path = f'{game_dir}/tagged_images/frame_{frame_id:06d}.jpg'\n",
    "        if not os.path.exists(frame_path):\n",
    "            continue\n",
    "\n",
    "        for sq in range(64):\n",
    "            game_rows.append({\n",
    "                'frame_path': frame_path,\n",
    "                'game_id': game_id,\n",
    "                'frame_id': frame_id,\n",
    "                'square_idx': sq,\n",
    "                'row': sq // 8,\n",
    "                'col': sq % 8,\n",
    "                'square_name': idx_to_square_name(sq),\n",
    "                'label_id': labels[sq],\n",
    "            })\n",
    "\n",
    "    game_df = pd.DataFrame(game_rows)\n",
    "    game_data[game_id] = game_df\n",
    "    \n",
    "    n_frames = game_df['frame_id'].nunique()\n",
    "    n_squares = len(game_df)\n",
    "    print(f\"{game_id}: {n_frames} frames, {n_squares:,} squares\")\n",
    "\n",
    "print(f\"\\nTotal games loaded: {len(game_data)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Phase 1: Create 2-fold cross-validation splits\n",
    "# Fold 1: Test on game7, Fold 2: Test on game5\n",
    "test_games_for_folds = [\n",
    "    sorted(game_data.keys())[-1],  # Last game alphabetically (game7)\n",
    "    sorted(game_data.keys())[2]    # Third game alphabetically (game5)\n",
    "]\n",
    "\n",
    "fold_manifests = []\n",
    "\n",
    "for fold_idx, test_game in enumerate(test_games_for_folds):\n",
    "    print(f\"\\nFold {fold_idx + 1}: Test on {test_game}\")\n",
    "    \n",
    "    # Combine 4 games for training (and validation split)\n",
    "    train_val_dfs = []\n",
    "    for game_id, game_df in game_data.items():\n",
    "        if game_id != test_game:\n",
    "            train_val_dfs.append(game_df.copy())\n",
    "    \n",
    "    train_val_df = pd.concat(train_val_dfs, ignore_index=True)\n",
    "    test_df = game_data[test_game].copy()\n",
    "    \n",
    "    # Split training data into 80% train, 20% val\n",
    "    unique_frames = train_val_df.groupby('game_id')['frame_id'].unique()\n",
    "    \n",
    "    train_frames_list = []\n",
    "    val_frames_list = []\n",
    "    \n",
    "    for game_id, frames in unique_frames.items():\n",
    "        frames = np.array(list(frames))\n",
    "        n_frames = len(frames)\n",
    "        \n",
    "        rng = np.random.RandomState(42)\n",
    "        rng.shuffle(frames)\n",
    "        \n",
    "        n_train = int(0.8 * n_frames)\n",
    "        train_frames_list.extend([(game_id, f) for f in frames[:n_train]])\n",
    "        val_frames_list.extend([(game_id, f) for f in frames[n_train:]])\n",
    "    \n",
    "    train_frame_set = set(train_frames_list)\n",
    "    val_frame_set = set(val_frames_list)\n",
    "    \n",
    "    def assign_split_train_val(row):\n",
    "        key = (row['game_id'], row['frame_id'])\n",
    "        if key in train_frame_set:\n",
    "            return 'train'\n",
    "        elif key in val_frame_set:\n",
    "            return 'val'\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    train_val_df['split'] = train_val_df.apply(assign_split_train_val, axis=1)\n",
    "    test_df['split'] = 'test'\n",
    "    \n",
    "    # Combine and save manifest for this fold\n",
    "    fold_df = pd.concat([train_val_df, test_df], ignore_index=True)\n",
    "    fold_df = fold_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    manifest_path = f'dataset_out/fold_{fold_idx + 1}_manifest.csv'\n",
    "    fold_df.to_csv(manifest_path, index=False)\n",
    "    \n",
    "    fold_manifests.append({\n",
    "        'fold': fold_idx + 1,\n",
    "        'test_game': test_game,\n",
    "        'manifest_path': manifest_path,\n",
    "        'train_squares': (fold_df['split'] == 'train').sum(),\n",
    "        'val_squares': (fold_df['split'] == 'val').sum(),\n",
    "        'test_squares': (fold_df['split'] == 'test').sum(),\n",
    "        'train_frames': fold_df[fold_df['split'] == 'train']['frame_id'].nunique(),\n",
    "        'val_frames': fold_df[fold_df['split'] == 'val']['frame_id'].nunique(),\n",
    "        'test_frames': fold_df[fold_df['split'] == 'test']['frame_id'].nunique(),\n",
    "    })\n",
    "    \n",
    "    print(f\"  Train: {fold_manifests[-1]['train_frames']} frames ({fold_manifests[-1]['train_squares']:,} sq)\")\n",
    "    print(f\"  Val:   {fold_manifests[-1]['val_frames']} frames ({fold_manifests[-1]['val_squares']:,} sq)\")\n",
    "    print(f\"  Test:  {fold_manifests[-1]['test_frames']} frames ({fold_manifests[-1]['test_squares']:,} sq) [from {test_game}]\")\n",
    "\n",
    "# Save fold summary\n",
    "fold_summary_df = pd.DataFrame(fold_manifests)\n",
    "fold_summary_df.to_csv('dataset_out/fold_summary.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2 FOLDS PREPARED (PROOF OF LEARNING)\")\n",
    "print(\"=\"*60)\n",
    "print(fold_summary_df.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Phase 2: Create manifest for training on ALL 5 games\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPARING ALL GAMES MANIFEST (FINAL TRAINING)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Combine all games\n",
    "all_games_dfs = [df.copy() for df in game_data.values()]\n",
    "all_games_df = pd.concat(all_games_dfs, ignore_index=True)\n",
    "\n",
    "# Split into 80% train, 20% val (no test set for final training)\n",
    "unique_frames = all_games_df.groupby('game_id')['frame_id'].unique()\n",
    "\n",
    "train_frames_list = []\n",
    "val_frames_list = []\n",
    "\n",
    "for game_id, frames in unique_frames.items():\n",
    "    frames = np.array(list(frames))\n",
    "    n_frames = len(frames)\n",
    "    \n",
    "    rng = np.random.RandomState(42)\n",
    "    rng.shuffle(frames)\n",
    "    \n",
    "    n_train = int(0.8 * n_frames)\n",
    "    train_frames_list.extend([(game_id, f) for f in frames[:n_train]])\n",
    "    val_frames_list.extend([(game_id, f) for f in frames[n_train:]])\n",
    "\n",
    "train_frame_set = set(train_frames_list)\n",
    "val_frame_set = set(val_frames_list)\n",
    "\n",
    "def assign_split_all_games(row):\n",
    "    key = (row['game_id'], row['frame_id'])\n",
    "    if key in train_frame_set:\n",
    "        return 'train'\n",
    "    elif key in val_frame_set:\n",
    "        return 'val'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "all_games_df['split'] = all_games_df.apply(assign_split_all_games, axis=1)\n",
    "all_games_df = all_games_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save manifest\n",
    "all_games_manifest_path = 'dataset_out/all_games_manifest.csv'\n",
    "all_games_df.to_csv(all_games_manifest_path, index=False)\n",
    "\n",
    "n_train = (all_games_df['split'] == 'train').sum()\n",
    "n_val = (all_games_df['split'] == 'val').sum()\n",
    "n_train_frames = all_games_df[all_games_df['split'] == 'train']['frame_id'].nunique()\n",
    "n_val_frames = all_games_df[all_games_df['split'] == 'val']['frame_id'].nunique()\n",
    "\n",
    "print(f\"Train: {n_train_frames} frames ({n_train:,} squares) - 80%\")\n",
    "print(f\"Val:   {n_val_frames} frames ({n_val:,} squares) - 20%\")\n",
    "print(f\"Total: {n_train + n_val:,} squares from all 5 games\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kRM-A4lJ0xSf",
    "outputId": "6ad5b611-b5d4-416a-bb63-67da32ca07e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "============================================================\n",
      "Creating dataloaders...\n",
      "============================================================\n",
      "Caching squares from 359 unique images...\n",
      "  Cached 20/359 images...\n",
      "  Cached 40/359 images...\n",
      "  Cached 60/359 images...\n",
      "  Cached 80/359 images...\n",
      "  Cached 100/359 images...\n",
      "  Cached 120/359 images...\n",
      "  Cached 140/359 images...\n",
      "  Cached 160/359 images...\n",
      "  Cached 180/359 images...\n",
      "  Cached 200/359 images...\n",
      "  Cached 220/359 images...\n",
      "  Cached 240/359 images...\n",
      "  Cached 260/359 images...\n",
      "  Cached 280/359 images...\n",
      "  Cached 300/359 images...\n",
      "  Cached 320/359 images...\n",
      "  Cached 340/359 images...\n",
      "✓ Cached 359 images with 22976 squares\n",
      "Dataset initialized: 23104 samples, 13 classes\n",
      "Created train loader: 23104 samples, 180 batches\n",
      "Dataset initialized: 4800 samples, 13 classes\n",
      "Created val loader: 4800 samples, 38 batches\n",
      "Dataset initialized: 5312 samples, 13 classes\n",
      "Created test loader: 5312 samples, 42 batches\n",
      "\n",
      "============================================================\n",
      "Creating model...\n",
      "============================================================\n",
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100% 97.8M/97.8M [00:01<00:00, 76.2MB/s]\n",
      "Created resnet50 model:\n",
      "  Pretrained: True\n",
      "  Num classes: 13\n",
      "  Final layer: Linear(2048 -> 13)\n",
      "Total parameters: 23,534,669\n",
      "\n",
      "============================================================\n",
      "Starting training...\n",
      "============================================================\n",
      "\n",
      "Epoch 1/15\n",
      "------------------------------------------------------------\n",
      "Epoch 1/15 [Train]: 100% 180/180 [03:37<00:00,  1.21s/it, loss=0.1308, acc=89.38%]\n",
      "Val: 100% 38/38 [00:23<00:00,  1.59it/s, loss=0.1487, acc=93.27%]\n",
      "\n",
      "Epoch 1 Summary:\n",
      "  Train Loss: 0.3199 | Train Acc: 89.38%\n",
      "  Val Loss:   0.2147 | Val Acc:   93.27%\n",
      "Saved checkpoint to checkpoints/best_model.pth\n",
      "  New best model! Val Acc: 93.27%\n",
      "Saved checkpoint to checkpoints/latest_model.pth\n",
      "\n",
      "Epoch 2/15\n",
      "------------------------------------------------------------\n",
      "Epoch 2/15 [Train]: 100% 180/180 [03:47<00:00,  1.26s/it, loss=0.0677, acc=95.62%]\n",
      "Val: 100% 38/38 [00:23<00:00,  1.61it/s, loss=0.0721, acc=93.65%]\n",
      "\n",
      "Epoch 2 Summary:\n",
      "  Train Loss: 0.1447 | Train Acc: 95.62%\n",
      "  Val Loss:   0.2839 | Val Acc:   93.65%\n",
      "Saved checkpoint to checkpoints/best_model.pth\n",
      "  New best model! Val Acc: 93.65%\n",
      "Saved checkpoint to checkpoints/latest_model.pth\n",
      "\n",
      "Epoch 3/15\n",
      "------------------------------------------------------------\n",
      "Epoch 3/15 [Train]:  51% 91/180 [01:57<01:54,  1.29s/it, loss=0.0922, acc=96.45%]\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/src/train.py\", line 318, in <module>\n",
      "    train(args)\n",
      "  File \"/content/src/train.py\", line 222, in train\n",
      "    train_loss, train_acc = train_epoch(\n",
      "                            ^^^^^^^^^^^^\n",
      "  File \"/content/src/train.py\", line 101, in train_epoch\n",
      "    running_loss += loss.item() * images.size(0)\n",
      "                    ^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE 1: 2-FOLD CROSS-VALIDATION (PROOF OF LEARNING)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Start Time: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Random Seed: 42 (for reproducibility)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load class mapping\n",
    "with open('dataset_out/classes.json', 'r') as f:\n",
    "    classes = json.load(f)\n",
    "num_classes = len(classes)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Load fold summary\n",
    "fold_summary = pd.read_csv('dataset_out/fold_summary.csv')\n",
    "print(f\"Number of folds: {len(fold_summary)}\")\n",
    "\n",
    "# Dataset class\n",
    "class ChessSquareDataset(Dataset):\n",
    "    def __init__(self, manifest_df, transform=None):\n",
    "        self.data = manifest_df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = row['frame_path']\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        x = int(row['col'] * (img.width / 8))\n",
    "        y = int(row['row'] * (img.height / 8))\n",
    "        w = int(img.width / 8)\n",
    "        h = int(img.height / 8)\n",
    "        \n",
    "        square_img = img.crop((x, y, x+w, y+h))\n",
    "        \n",
    "        if self.transform:\n",
    "            square_img = self.transform(square_img)\n",
    "        \n",
    "        label = row['label_id']\n",
    "        return square_img, label\n",
    "\n",
    "# Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Training functions\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Validating\", leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nDevice: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 8\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "print(f\"\\nHyperparameters:\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Workers: {NUM_WORKERS}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Results storage\n",
    "all_results = []\n",
    "\n",
    "# Train only 2 folds for proof of learning\n",
    "for fold_idx in range(len(fold_summary)):\n",
    "    fold_num = fold_summary.iloc[fold_idx]['fold']\n",
    "    test_game = fold_summary.iloc[fold_idx]['test_game']\n",
    "    manifest_path = fold_summary.iloc[fold_idx]['manifest_path']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FOLD {fold_num}/{len(fold_summary)}\")\n",
    "    print(f\"Test Game: {test_game}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load manifest\n",
    "    manifest_df = pd.read_csv(manifest_path)\n",
    "    \n",
    "    train_df = manifest_df[manifest_df['split'] == 'train']\n",
    "    val_df = manifest_df[manifest_df['split'] == 'val']\n",
    "    test_df = manifest_df[manifest_df['split'] == 'test']\n",
    "    \n",
    "    print(f\"Train: {len(train_df):,} squares\")\n",
    "    print(f\"Val:   {len(val_df):,} squares\")\n",
    "    print(f\"Test:  {len(test_df):,} squares\")\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = ChessSquareDataset(train_df, transform=train_transform)\n",
    "    val_dataset = ChessSquareDataset(val_df, transform=val_transform)\n",
    "    test_dataset = ChessSquareDataset(test_df, transform=val_transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    # Initialize model (fresh ResNet50 for each fold)\n",
    "    model = models.resnet50(weights='IMAGENET1K_V1')\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    fold_start_time = time.time()\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_start = time.time()\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        \n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f}, Val Acc:   {val_acc:.2f}%\")\n",
    "        print(f\"  Epoch Time: {epoch_time:.1f}s\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch + 1\n",
    "            \n",
    "            checkpoint_path = f'dataset_out/best_model_fold_{fold_num}.pth'\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch + 1,\n",
    "                'val_acc': val_acc,\n",
    "                'fold': fold_num,\n",
    "                'test_game': test_game\n",
    "            }, checkpoint_path, _use_new_zipfile_serialization=False)\n",
    "            print(f\"  ✓ Best model saved! (Val Acc: {val_acc:.2f}%)\")\n",
    "    \n",
    "    # Load best model and evaluate on test set\n",
    "    print(f\"\\n--- Testing Fold {fold_num} ---\")\n",
    "    checkpoint = torch.load(f'dataset_out/best_model_fold_{fold_num}.pth', weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    test_loss, test_acc = validate(model, test_loader, criterion, device)\n",
    "    \n",
    "    fold_time = time.time() - fold_start_time\n",
    "    \n",
    "    print(f\"Best Val Acc:  {best_val_acc:.2f}% (Epoch {best_epoch})\")\n",
    "    print(f\"Test Acc:      {test_acc:.2f}%\")\n",
    "    print(f\"Fold Duration: {fold_time/60:.1f} minutes\")\n",
    "    \n",
    "    all_results.append({\n",
    "        'fold': fold_num,\n",
    "        'test_game': test_game,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'best_epoch': best_epoch,\n",
    "        'test_acc': test_acc,\n",
    "        'time_minutes': fold_time / 60\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2-FOLD CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nAverage Validation Accuracy: {results_df['best_val_acc'].mean():.2f}%\")\n",
    "print(f\"Average Test Accuracy:       {results_df['test_acc'].mean():.2f}%\")\n",
    "print(f\"Total Training Time:         {results_df['time_minutes'].sum():.1f} minutes\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('dataset_out/fold_results.csv', index=False)\n",
    "print(\"\\n✓ 2-fold validation complete! Results saved to dataset_out/fold_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 2: TRAINING ON ALL 5 GAMES (FINAL MODEL)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Start Time: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Load all games manifest\n",
    "all_manifest = pd.read_csv('dataset_out/all_games_manifest.csv')\n",
    "train_df = all_manifest[all_manifest['split'] == 'train']\n",
    "val_df = all_manifest[all_manifest['split'] == 'val']\n",
    "\n",
    "print(f\"\\nTrain: {len(train_df):,} squares (80% of all 5 games)\")\n",
    "print(f\"Val:   {len(val_df):,} squares (20% of all 5 games)\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ChessSquareDataset(train_df, transform=train_transform)\n",
    "val_dataset = ChessSquareDataset(val_df, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Initialize fresh model for all games\n",
    "model = models.resnet50(weights='IMAGENET1K_V1')\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "history = []\n",
    "train_start = time.time()\n",
    "\n",
    "print(\"\\nTraining for 8 epochs...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    history.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'train_acc': train_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'val_acc': val_acc,\n",
    "        'lr': optimizer.param_groups[0]['lr'],\n",
    "        'time_sec': epoch_time\n",
    "    })\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f}, Val Acc:   {val_acc:.2f}%\")\n",
    "    print(f\"  Epoch Time: {epoch_time:.1f}s\")\n",
    "    print(f\"  LR:         {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'train_acc': train_acc,\n",
    "            'games': 'all_5_games'\n",
    "        }, 'dataset_out/best_model_all_games.pth', _use_new_zipfile_serialization=False)\n",
    "        print(f\"  ✓ Best model saved! (Val Acc: {val_acc:.2f}%)\")\n",
    "\n",
    "train_time = time.time() - train_start\n",
    "\n",
    "# Save training history\n",
    "pd.DataFrame(history).to_csv('dataset_out/all_games_history.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING ON ALL 5 GAMES COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best Val Acc:    {best_val_acc:.2f}% (epoch {best_epoch})\")\n",
    "print(f\"Training Time:   {train_time/60:.1f} minutes\")\n",
    "print(f\"Model Saved:     dataset_out/best_model_all_games.pth\")\n",
    "print(\"\\n✓ Final production model ready!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the final trained model\n",
    "from google.colab import files\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DOWNLOADING FINAL MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_path = 'dataset_out/best_model_all_games.pth'\n",
    "print(f\"\\nDownloading: {model_path}\")\n",
    "files.download(model_path)\n",
    "\n",
    "print(\"\\n✓ Model downloaded!\")\n",
    "print(\"\\nTo use in your web app:\")\n",
    "print(\"1. Rename to: best_model_fold_1.pth\")\n",
    "print(\"2. Place in: checkpoints/ folder\")\n",
    "print(\"3. Run: python app.py\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
